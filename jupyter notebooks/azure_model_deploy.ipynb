{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Environment (conda_tensorflow_p36)",
      "language": "python",
      "name": "conda_tensorflow_p36"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "azure_deploy.ipynb",
      "version": "0.3.2",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvDAEPQU0N8c",
        "colab_type": "text"
      },
      "source": [
        "# Azure Model Deploy\n",
        "In this notebook we will deploy our Neural Image Captioning Model to an Azure Web Service using Azure ML API for Python, at the end of this notebook we will have a single endpoint we can query with a POST Request to caption an image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuIf3LDZ0GP7",
        "colab_type": "text"
      },
      "source": [
        "## Dependencies\n",
        "Let's begin installing the modules *azureml* and *azureml-core* using pip."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maaCHv1H0GP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install azureml\n",
        "!pip install azureml-core"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7zbN0ve0GQE",
        "colab_type": "text"
      },
      "source": [
        "## Create the Workspace\n",
        "First, we have to create a new Workspace, put your Azure Subscription ID inside *subscription_id*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lco5lQlp0GQG",
        "colab_type": "code",
        "outputId": "41e96502-0005-4857-fa62-50a8e8cf5aeb",
        "colab": {}
      },
      "source": [
        "from azureml.core import Workspace\n",
        "\n",
        "ws = Workspace.create(name='ImageCaptioningWorkspace',\n",
        "                      subscription_id='<YOUR_SUBSCRIPTION_ID'>,\n",
        "                      resource_group='M',\n",
        "                      create_resource_group=True,\n",
        "                      location='eastus2', \n",
        "                      exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Deploying KeyVault with name imagecapkeyvaultd350883b.\n",
            "Deploying StorageAccount with name imagecapstorage930027103.\n",
            "Deploying AppInsights with name imagecapinsights4fcdb66c.\n",
            "Deployed AppInsights with name imagecapinsights4fcdb66c. Took 7.31 seconds.\n",
            "Deployed KeyVault with name imagecapkeyvaultd350883b. Took 27.69 seconds.\n",
            "Deployed StorageAccount with name imagecapstorage930027103. Took 31.65 seconds.\n",
            "Deploying Workspace with name ImageCaptioningWorkspace.\n",
            "Deployed Workspace with name ImageCaptioningWorkspace. Took 58.62 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16bGINsV0GQK",
        "colab_type": "text"
      },
      "source": [
        "## Register the Model\n",
        "Our entire model is composed by three sub-models:\n",
        "* The tokenizer: in pickle format\n",
        "* The image encoding model: encoded in h5 format\n",
        "* The image captioning model: encoded in h5 format\n",
        "\n",
        "We saved the models into the folder *model*, now it's time to register our Model using this directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNkQTaas0GQL",
        "colab_type": "code",
        "outputId": "660cd143-df59-4904-b828-91f760576d23",
        "colab": {}
      },
      "source": [
        "from azureml.core.model import Model\n",
        "\n",
        "model = Model.register(model_path = \"./model\",\n",
        "                       model_name = \"ImageCaptioningModel\",\n",
        "                       description = \"An Image Captioning model\",\n",
        "                       workspace = ws)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Registering model ImageCaptioningModel\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTnWMh_W0GQN",
        "colab_type": "text"
      },
      "source": [
        "## Define the Estimator\n",
        "We have to define a script that will be executed when our API is called, this script contains just two functions:\n",
        "* **init**: it is used to load models and initialize stuff that we need (if we need)\n",
        "* **run** it receives the request' payload in json format, which is supposed to contain the image encoded in some way, and here we will use our models to process inputs and generate captions.\n",
        "\n",
        "We will accept images both from URL and list of flattened pixels. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3Nlmfov0GQN",
        "colab_type": "code",
        "outputId": "c62de679-294d-4da9-a7f0-1e34173f5024",
        "colab": {}
      },
      "source": [
        "%%writefile caption.py\n",
        "\n",
        "import pickle\n",
        "import json\n",
        "import sys\n",
        "\n",
        "from azureml.core.model import Model\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from keras.models import load_model\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.image import img_to_array\n",
        "\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "import traceback\n",
        "\n",
        "\n",
        "def init():\n",
        "    \n",
        "    global caption_model\n",
        "    global tokenizer\n",
        "    global encode_model\n",
        "    global model_path\n",
        "    \n",
        "    global MAX_LEN\n",
        "    global OUTPUT_DIM\n",
        "    global WIDTH\n",
        "    global HEIGHT\n",
        "    \n",
        "    MAX_LEN = 46\n",
        "    OUTPUT_DIM = 2048\n",
        "    WIDTH = 299\n",
        "    HEIGHT = 299\n",
        "        \n",
        "    model_path = Model.get_model_path('ImageCaptioningModel')\n",
        "    caption_model = load_model(model_path+\"/caption_model.h5\")\n",
        "    encode_model = load_model(model_path+\"/encode_model.h5\")\n",
        "    \n",
        "    with open(model_path + '/tokenizer.pickle','rb') as handle:\n",
        "        tokenizer = pickle.load(handle)\n",
        "\n",
        "\n",
        "\n",
        "def run(raw_data):\n",
        "    try:\n",
        "        \n",
        "        caption = \"startseq\"\n",
        "        \n",
        "        data = json.loads(raw_data)\n",
        "         \n",
        "        if(\"url\" in data):\n",
        "          \n",
        "           # payload contains an URL\n",
        "           # download the image\n",
        "          Â # and convert to an array\n",
        "          \n",
        "            url = data[\"url\"]\n",
        "                \n",
        "            response = requests.get(url)\n",
        "            img = Image.open(BytesIO(response.content))\n",
        "        \n",
        "            img = img.resize((WIDTH, HEIGHT), Image.ANTIALIAS)  \n",
        "            img = img_to_array(img)\n",
        "            \n",
        "        elif(\"data\" in data):\n",
        "          \n",
        "            # payload contains a flattend array\n",
        "            # reshape it with the correct dimensions\n",
        "          \n",
        "            arr = np.array(data[\"data\"], dtype=np.float32)\n",
        "            img = arr.reshape((WIDTH,HEIGHT, 3))\n",
        "        else:\n",
        "            # if no 'data' or 'url' is defined\n",
        "            # return an error\n",
        "            return {\"error\":\"No data provided\"}\n",
        "        \n",
        "        # preprocess the image\n",
        "        \n",
        "        img = preprocess_input(img)\n",
        "        img = np.expand_dims(img, axis=0)\n",
        "        \n",
        "        x1 = encode_model.predict(img)\n",
        "        x1 = x1.reshape((1, OUTPUT_DIM))\n",
        "        \n",
        "        # generate the caption\n",
        "    \n",
        "        for i in range(MAX_LEN):\n",
        "            seq = tokenizer.texts_to_sequences([caption])\n",
        "            x2 = pad_sequences(seq, maxlen=MAX_LEN)\n",
        "        \n",
        "            y = caption_model.predict([x1,x2], verbose=0)\n",
        "            word = tokenizer.index_word[np.argmax(y)]\n",
        "        \n",
        "            if word == \"endseq\":\n",
        "                break\n",
        "      \n",
        "            caption+=\" \"+word\n",
        "    \n",
        "        caption = caption.replace(\"startseq\",\"\").strip()\n",
        "        return {\"caption\":caption}\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(traceback.format_exc())\n",
        "        return {\"error\":str(e)}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting caption.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MjNM5Ax0GQP",
        "colab_type": "text"
      },
      "source": [
        "## Define the Environment\n",
        "\n",
        "To run our model we need to define the envirnment in which it will run, we can do this with a yml file. We can generate this file using the azureml's class *CondaDependencies*. Our model needs the following dependencies:\n",
        "\n",
        "* **Numpy**: for arrays manipulation\n",
        "* **Pillow**: for images processing\n",
        "* **Keras**: to load and use the neural network\n",
        "* **Tensorflow**: to run keras.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKFKeR240GQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from azureml.core.conda_dependencies import CondaDependencies \n",
        "\n",
        "myenv = CondaDependencies()\n",
        "myenv.add_pip_package(\"numpy\")\n",
        "myenv.add_pip_package(\"azureml-core\")\n",
        "myenv.add_pip_package(\"Pillow\")\n",
        "myenv.add_pip_package(\"keras\")\n",
        "myenv.add_pip_package(\"tensorflow\")\n",
        "\n",
        "with open(\"mlenv.yml\",\"w\") as f:\n",
        "    f.write(myenv.serialize_to_string())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsAaREcY0GQT",
        "colab_type": "text"
      },
      "source": [
        "## Create the Container\n",
        "Azure deploy ML models in a Docker container, let's build the image for the container."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PW0EFhAJ0GQU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from azureml.core.image import ContainerImage\n",
        "\n",
        "image_config = ContainerImage.image_configuration(execution_script = \"caption.py\",\n",
        "                                                  runtime = \"python\",\n",
        "                                                  conda_file = \"mlenv.yml\",\n",
        "                                                  description = \"A Neural Image Captioning Model Image\"\n",
        "                                                 )\n",
        "\n",
        "image = ContainerImage.create(name = \"image-captioning-container\",\n",
        "                              models = [model],\n",
        "                              image_config = image_config,\n",
        "                              workspace = ws)\n",
        "\n",
        "image.wait_for_creation(show_output = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9VrkEcg0GQZ",
        "colab_type": "text"
      },
      "source": [
        "## Deploy the Model\n",
        "\n",
        "It's time to deploy ! We configure our VM with 1 single CPU core and 1 GB of virtual memory, it will be enough."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IrmbrQo0GQZ",
        "colab_type": "code",
        "outputId": "f8040703-186b-4c9e-ec6d-6c2134571784",
        "colab": {}
      },
      "source": [
        "from azureml.core.webservice import AciWebservice, Webservice\n",
        "\n",
        "aciconfig = AciWebservice.deploy_configuration(cpu_cores = 1, \n",
        "                                               memory_gb = 1, \n",
        "                                               tags = {\"data\": \"image-captioning\", \"type\": \"classification\"}, \n",
        "                                               description = 'An Image Captioning Model')\n",
        "\n",
        "service = Webservice.deploy_from_image(deployment_config = aciconfig,\n",
        "                                            image = image,\n",
        "                                            name = \"image-captioning\",\n",
        "                                            workspace = ws)\n",
        "\n",
        "\n",
        "\n",
        "service.wait_for_deployment(show_output = True) # wait until deploy is completed"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error, there is already a service with name image-captioning found in workspace ImageCaptioningWorkspace\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "WebserviceException",
          "evalue": "WebserviceException:\n\tMessage: Error, there is already a service with name image-captioning found in workspace ImageCaptioningWorkspace\n\tInnerException None\n\tErrorResponse {\"error\": {\"message\": \"Error, there is already a service with name image-captioning found in workspace ImageCaptioningWorkspace\"}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mWebserviceException\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/azureml/core/webservice/webservice.py\u001b[0m in \u001b[0;36m_check_for_existing_webservice\u001b[0;34m(workspace, name)\u001b[0m\n\u001b[1;32m    361\u001b[0m             raise WebserviceException('Error, there is already a service with name {} found in '\n\u001b[0;32m--> 362\u001b[0;31m                                       'workspace {}'.format(name, workspace._workspace_name))\n\u001b[0m\u001b[1;32m    363\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mWebserviceException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mWebserviceException\u001b[0m: WebserviceException:\n\tMessage: Error, there is already a service with name image-captioning found in workspace ImageCaptioningWorkspace\n\tInnerException None\n\tErrorResponse {\"error\": {\"message\": \"Error, there is already a service with name image-captioning found in workspace ImageCaptioningWorkspace\"}}",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mWebserviceException\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-4f10db06efdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                                             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"image-captioning\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                                             workspace = ws)\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \"\"\"\n",
            "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/azureml/core/webservice/webservice.py\u001b[0m in \u001b[0;36mdeploy_from_image\u001b[0;34m(workspace, name, image, deployment_config, deployment_target)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mwebservice_name_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0mWebservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_for_local_deployment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeployment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0mWebservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_for_existing_webservice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeployment_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdeployment_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/azureml/core/webservice/webservice.py\u001b[0m in \u001b[0;36m_check_for_existing_webservice\u001b[0;34m(workspace, name)\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mWebserviceException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'WebserviceNotFound'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mWebserviceException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodule_logger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mWebserviceException\u001b[0m: WebserviceException:\n\tMessage: Error, there is already a service with name image-captioning found in workspace ImageCaptioningWorkspace\n\tInnerException None\n\tErrorResponse {\"error\": {\"message\": \"Error, there is already a service with name image-captioning found in workspace ImageCaptioningWorkspace\"}}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfjGHkwp9O9T",
        "colab_type": "text"
      },
      "source": [
        "If we need we can print our service's log using the method *.get_logs()*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "bOHzy1FX0GQb",
        "colab_type": "code",
        "outputId": "a389f357-3a5c-4e76-b241-4b4ec66be967",
        "colab": {}
      },
      "source": [
        "service.get_logs()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2019-09-01T16:24:14,710416826+00:00 - gunicorn/run \\n2019-09-01T16:24:14,716812803+00:00 - iot-server/run \\n2019-09-01T16:24:14,731088576+00:00 - nginx/run \\n2019-09-01T16:24:14,747645675+00:00 - rsyslog/run \\nEdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\\n2019-09-01T16:24:15,154286578+00:00 - iot-server/finish 1 0\\n2019-09-01T16:24:15,206283304+00:00 - Exit code 1 is normal. Not restarting iot-server.\\nStarting gunicorn 19.9.0\\nListening at: http://127.0.0.1:31311 (10)\\nUsing worker: sync\\nworker timeout is set to 300\\nBooting worker with pid: 45\\nInitializing logger\\nStarting up app insights client\\nStarting up request id generator\\nStarting up app insight hooks\\nInvoking user\\'s init function\\n2019-09-01 16:24:23,552 | azureml.core.run | DEBUG | Could not load run context RunEnvironmentException:\\n\\tMessage: Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\\n\\tInnerException None\\n\\tErrorResponse \\n{\\n    \"error\": {\\n        \"message\": \"Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\"\\n    }\\n}, switching offline: False\\n2019-09-01 16:24:23,552 | azureml.core.run | DEBUG | Could not load the run context and allow_offline set to False\\n2019-09-01 16:24:23,552 | azureml.core.model | DEBUG | RunEnvironmentException: RunEnvironmentException:\\n\\tMessage: Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\\n\\tInnerException RunEnvironmentException:\\n\\tMessage: Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\\n\\tInnerException None\\n\\tErrorResponse \\n{\\n    \"error\": {\\n        \"message\": \"Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\"\\n    }\\n}\\n\\tErrorResponse \\n{\\n    \"error\": {\\n        \"message\": \"Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\"\\n    }\\n}\\n2019-09-01 16:24:23,552 | azureml.core.model | DEBUG | Checking root for ImageCaptioning because candidate dir azureml-models had 3 nodes: azureml-models/ImageCaptioningModel/1/model/encode_model.h5\\nazureml-models/ImageCaptioningModel/1/model/tokenizer.pickle\\nazureml-models/ImageCaptioningModel/1/model/caption_model.h5\\nUser\\'s init function failed\\nUsing TensorFlow backend.\\n/opt/miniconda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or \\'1type\\' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \\'(1,)type\\'.\\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\\n/opt/miniconda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or \\'1type\\' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \\'(1,)type\\'.\\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\\n/opt/miniconda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or \\'1type\\' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \\'(1,)type\\'.\\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\\n/opt/miniconda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or \\'1type\\' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \\'(1,)type\\'.\\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\\n/opt/miniconda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or \\'1type\\' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \\'(1,)type\\'.\\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\\n/opt/miniconda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or \\'1type\\' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \\'(1,)type\\'.\\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\\n/opt/miniconda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or \\'1type\\' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \\'(1,)type\\'.\\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\\n/opt/miniconda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or \\'1type\\' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \\'(1,)type\\'.\\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\\n/opt/miniconda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or \\'1type\\' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \\'(1,)type\\'.\\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\\n/opt/miniconda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or \\'1type\\' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \\'(1,)type\\'.\\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\\n/opt/miniconda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or \\'1type\\' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \\'(1,)type\\'.\\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\\n/opt/miniconda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or \\'1type\\' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / \\'(1,)type\\'.\\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\\nEncountered Exception Traceback (most recent call last):\\n  File \"/var/azureml-server/aml_blueprint.py\", line 162, in register\\n    main.init()\\n  File \"/var/azureml-app/main.py\", line 88, in init\\n    driver_module.init()\\n  File \"caption.py\", line 40, in init\\n    model_path = Model.get_model_path(\\'ImageCaptioning\\')\\n  File \"/opt/miniconda/lib/python3.6/site-packages/azureml/core/model.py\", line 501, in get_model_path\\n    return Model._get_model_path_local(model_name, version)\\n  File \"/opt/miniconda/lib/python3.6/site-packages/azureml/core/model.py\", line 522, in _get_model_path_local\\n    return Model._get_model_path_local_from_root(model_name)\\n  File \"/opt/miniconda/lib/python3.6/site-packages/azureml/core/model.py\", line 565, in _get_model_path_local_from_root\\n    \"set logging level to DEBUG.\".format(candidate_model_path))\\nazureml.exceptions._azureml_exception.ModelNotFoundException: ModelNotFoundException:\\n\\tMessage: Model not found in cache or in root at ./ImageCaptioning. For more info,set logging level to DEBUG.\\n\\tInnerException None\\n\\tErrorResponse \\n{\\n    \"error\": {\\n        \"message\": \"Model not found in cache or in root at ./ImageCaptioning. For more info,set logging level to DEBUG.\"\\n    }\\n}\\n\\nWorker exiting (pid: 45)\\nShutting down: Master\\nReason: Worker failed to boot.\\n2019-09-01T16:24:23,814628441+00:00 - gunicorn/finish 3 0\\n2019-09-01T16:24:23,815886156+00:00 - Exit code 3 is not normal. Killing image.\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAW8oC0t9V1c",
        "colab_type": "text"
      },
      "source": [
        "If we have already created the service and we need to update it, then we can use the method *.update(image)* with the new image to deploy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XG16tTuM0GQd",
        "colab_type": "code",
        "outputId": "edbaa9ea-69f6-43a4-e941-eb3c12c2c5e5",
        "colab": {}
      },
      "source": [
        "service = Webservice(name=\"image-captioning\", workspace = ws)\n",
        "service.update(image=image)\n",
        "\n",
        "service.wait_for_deployment(show_output = True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running..."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7hMOM8O0GQf",
        "colab_type": "text"
      },
      "source": [
        "## Test the Web Service\n",
        "Let's test our web service making a POST request to our service endpoint, first using a link to an image taken from Google Images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntvgQjnW0GQh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "9584d15a-d318-419c-e337-72ce257d4474"
      },
      "source": [
        "import json\n",
        "import requests\n",
        "\n",
        "IMG_URL = \"https://cdn.pixabay.com/photo/2017/02/20/18/03/cat-2083492_960_720.jpg\"\n",
        "\n",
        "input_data = json.dumps({\"url\": IMG_URL})\n",
        "\n",
        "headers = {'Content-Type':'application/json'}\n",
        "\n",
        "resp = requests.post(service.scoring_uri, input_data, headers=headers)\n",
        "\n",
        "print(\"POST to url\", service.scoring_uri)\n",
        "print(json.loads(resp.text))\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8444f94f1e25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'Content-Type'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'application/json'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscoring_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"POST to url\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscoring_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'service' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQbrfjmW0GQj",
        "colab_type": "text"
      },
      "source": [
        "and then with an image flattened in a list of pixel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiS0WCW00GQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://images.financialexpress.com/2018/12/train-18-tracks-660.jpg\n",
        "\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from PIL import Image\n",
        "\n",
        "import json\n",
        "import requests\n",
        "\n",
        "\n",
        "img = Image.open(\"train-18-tracks-660.jpg\")\n",
        "img = img.resize((299, 299), Image.ANTIALIAS)  \n",
        "img = img_to_array(img)\n",
        "\n",
        "input_data = json.dumps({\"data\": img.tolist()})\n",
        "headers = {'Content-Type':'application/json'}\n",
        "resp = requests.post(service.scoring_uri, input_data, headers=headers)\n",
        "\n",
        "print(\"POST to url\", service.scoring_uri)\n",
        "print(json.loads(resp.text))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qu7YC_Rz0GQo",
        "colab_type": "text"
      },
      "source": [
        "### Our model is alive !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hG5vRYGpQBc",
        "colab_type": "text"
      },
      "source": [
        "# How to improve our App\n",
        "I use the web service we have just deployed from a mobile App called Pic2Speech, you can find it on [Google Play](https://play.google.com/store/apps/details?id=gfg.app.pictospeech). The app is still really simple, it just take a picture and call the APi to caption it, since the problem's complexity the app doesn't always return accurate results but it identificate the context of a picture most of the time. In future I would like to add the following functionalities to improve the app:\n",
        "\n",
        "1. In the current status the app doesn't store pictures taken from users, in future would be great to ask users for feedbacks on generated captions and use that information to improve the model using **continual learning**.\n",
        "1. Allow users to manually caption pictures taken by themself or by other users, who accepted to share their pictures with others, to improve the model using **continual learning**.\n",
        "2. Add a metric to evaluate generated captions' accuracy, in the case of low accuracy the app will use a InceptionV3 model, even on device using Tensorflow Lite, to simply list the objects detected in the pictures. Then we could ask users to manually caption such pictures as in point 1."
      ]
    }
  ]
}