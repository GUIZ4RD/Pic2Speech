{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image_captioning_keras.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bH_6azLLtROg",
        "colab_type": "text"
      },
      "source": [
        "# Pic2Speech - A Neural Image Captioning Model\n",
        "In this Notebook we are going to create a Neural Image Captioning Model. Our model will be able to generate a description for a given picture. After building the model we will deploy it to a Web Service using Azure Machine Learning service and we will use it from a mobile app. I called the app Pic2Speech and it's purpose is to help visually impaired understanding the world, describing the content of pictures taken with smartphone's camera through a TTS (Text To Speech) synthesizer. The app is fully Open Source and it's available for download on [Google Play](https://play.google.com/store/apps/details?id=gfg.app.pictospeech)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgJHzroq59WO",
        "colab_type": "text"
      },
      "source": [
        "## The Dataset\n",
        "The dataset we will use to train our model is the MS COCO 2019 Dataset. This dataset contains 82783 pictures with 5 human generated captions each. Let's download the full dataset from its orignal repository and extract the zip archives with unzip."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WdQOJ0z-XZy",
        "colab_type": "code",
        "outputId": "ba8803d0-59dc-4f82-c0d3-4ec863682240",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
        "!wget http://images.cocodataset.org/zips/train2014.zip\n",
        "  \n",
        "!unzip -q annotations_trainval2014.zip\n",
        "!unzip -q train2014.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "replace annotations/instances_train2014.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "A\n",
            "replace train2014/COCO_train2014_000000270070.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2B7lvCF-7UM",
        "colab_type": "text"
      },
      "source": [
        "Inside *annotations* directory we can find a JSON file called *captions_train2014.json*, it contains all the captions in the following format:\n",
        "<br><br>\n",
        "*{<br>\n",
        "&nbsp;\"annotations\": <br>\n",
        "&nbsp; { <br>\n",
        "&emsp;\"image_id\": IMAGE_ID, <br>\n",
        "&emsp;\"caption\": CAPTION} <br>\n",
        "&nbsp;},<br>\n",
        "&nbsp;...<br>\n",
        "}*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOpg4qOlCJ7X",
        "colab_type": "text"
      },
      "source": [
        "Where IMAGE_ID is the id of the image to which the caption refers and CAPTION is the caption. Inside the folder train2014 we can find the actual images in jpg format, image are named with this standard\n",
        "<br><br>\n",
        "*COCO_train2014_**IMAGE_ID**.jpg*\n",
        "<br><br>\n",
        "in this case IMAGE_ID is always a 14 characters long string where the last characters are the actual IMAGE_ID and all the others are 0. For example, an IMAGE_ID of 317672 in captions_train2014.json will refer to the image *COCO_train2014_0000000317672.jpg*.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0U4Ojh6F8ks0",
        "colab_type": "text"
      },
      "source": [
        "## The Model\n",
        "The model we are going to build is mostly based on the paper \"Show and Tell: A Neural Image Caption Generator\" by Vinyals et al [(1)](#references). The model consists of:\n",
        "* A Deep Convolutional Neural Network used to encode the image in lower dimensional space.\n",
        "* A Language Generating-Recurrent Neural Network that take in input the encoded image and generate a caption for it using the sampling tecnique, where we sample the next word until we get the end of sequence character or until we reach the maximum length for a caption.\n",
        "<br><br>\n",
        "<img src=\"https://github.com/gfgullo/Pic2Speech/raw/master/jupyter%20notebooks/res/nicm_1.png\"/>\n",
        "<p align=\"center\">Image from <a href=\"https://arxiv.org/pdf/1411.4555.pdf\" target=\"_blank\">Show and Tell: A Neural Image Caption Generator</a> by Vinyals et al</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPK-dnC4aHIZ",
        "colab_type": "text"
      },
      "source": [
        "## Dependencies\n",
        "We will create the model using **Keras** on top of **Tensorflow 1.14**, we will also use some scikit-learn's functions for data preprocessing and model selection. We will use **BLEU (2)** for model evaluation, a scoring method mostly used in Machine Translation that will fit good also for our purpose, we will use it through **NLTK**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Se7cKMyzaJ2D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import json\n",
        "import string\n",
        "import pickle\n",
        "import os\n",
        "from time import time\n",
        "from random import randint\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.models import load_model\n",
        "\n",
        "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "from keras.layers import Input, Dropout, Embedding, LSTM, Dense\n",
        "from keras.layers.merge import add\n",
        "\n",
        "from keras.callbacks import LambdaCallback, EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "from IPython.display import display"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZF-bVe4B5-l",
        "colab_type": "text"
      },
      "source": [
        "## Preprocess the Text\n",
        "Let's begin preprocessing all the captions. We can limit the number of images we want use to train our model putting the number inside *img_count*, this could be useful if we don't have enough computing capability to train the model on the entire dataset. Of course the more samples we use the better the model's quality we can achieve. If you want to train the model on the entire dataset just put None.<br>\n",
        "We will process every caption applying the following transformations:\n",
        "* Convert the entire string to lower case.\n",
        "* Remove all punctuation from the string.\n",
        "* Add string delimiters using *startseq* and *endseq*, we have to do this because our model need to know when a caption end and when the next begin.\n",
        "\n",
        "We will save captions in a dictionary where the key is the image id."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vMBpw075vGt",
        "colab_type": "code",
        "outputId": "b421515d-e9c3-412b-c69a-74b3ca04437a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "CAPTIONS_FILE = \"annotations/captions_train2014.json\"\n",
        "\n",
        "imgs_count = None # use this variable to limit the number of samples\n",
        "\n",
        "img_captions = {}\n",
        "\n",
        "with open(CAPTIONS_FILE) as captions_file:\n",
        "  \n",
        "  captions_json = json.loads(captions_file.read())\n",
        "  \n",
        "  for caption in captions_json[\"annotations\"]:\n",
        "        \n",
        "    caption_text = caption[\"caption\"]\n",
        "    caption_text = caption_text.lower() # convert the text in lowercase\n",
        "    caption_text = caption_text.translate(str.maketrans('', '', string.punctuation)) # remove punctuation\n",
        "    caption_text = \"startseq \"+caption_text+\" endseq\" # add start and end of sequence delimiter\n",
        "    \n",
        "    img_id = caption[\"image_id\"]\n",
        "\n",
        "    if(img_id in img_captions):\n",
        "      img_captions[img_id].append(caption_text)\n",
        "    else:\n",
        "      img_captions[img_id] = [caption_text]\n",
        "      if(imgs_count!=None):\n",
        "        imgs_count-=1\n",
        "        if(imgs_count <= 0):\n",
        "          break\n",
        "\n",
        "len(img_captions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77Fq_x1YEpAt",
        "colab_type": "text"
      },
      "source": [
        "Now we can use the dictionary to create two separated lists for images id and for captions, in this way all the captions that refers to the same image will be close to each other in the lists. This is important because, by doing this, when we split the sets for train and test, we will be sure that images that will be used for test aren't also in the train set, so the evaluation on the test set will be much more reliable. Of course to train the model we have to shuffle the train set, but we can do this later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz_5knxyTBFR",
        "colab_type": "code",
        "outputId": "1436d915-b078-4a33-ade0-a521359d48f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "captions_text, captions_img = [], []\n",
        "\n",
        "for img_id in img_captions:\n",
        "  for caption in img_captions[img_id]:\n",
        "    captions_img.append(img_id)\n",
        "    captions_text.append(caption)\n",
        "\n",
        "print(captions_text[:20])\n",
        "print(captions_img[:20])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['startseq a very clean and well decorated empty bathroom endseq', 'startseq a blue and white bathroom with butterfly themed wall tiles endseq', 'startseq a bathroom with a border of butterflies and blue paint on the walls above it endseq', 'startseq an angled view of a beautifully decorated bathroom endseq', 'startseq a clock that blends in with the wall hangs in a bathroom  endseq', 'startseq a panoramic view of a kitchen and all of its appliances endseq', 'startseq a panoramic photo of a kitchen and dining room endseq', 'startseq a wide angle view of the kitchen work area endseq', 'startseq multiple photos of a brown and white kitchen  endseq', 'startseq a graffitied stop sign across the street from a red car  endseq', 'startseq a vandalized stop sign and a red beetle on the road endseq', 'startseq a red stop sign with a bush bumper sticker under the word stop endseq', 'startseq a stop sign that has been vandalized is pictured in front of a parked car endseq', 'startseq a street sign modified to read stop bush endseq', 'startseq the two people are walking down the beach endseq', 'startseq two people carrying surf boards on a beach endseq', 'startseq two teenagers at a white sanded beach with surfboards endseq', 'startseq a couple at the beach walking with their surf boards endseq', 'startseq a guy and a girl are walking on the beach holding surfboards endseq', 'startseq a sink and a toilet inside a small bathroom endseq']\n",
            "[318556, 318556, 318556, 318556, 318556, 116100, 116100, 116100, 116100, 379340, 379340, 379340, 379340, 379340, 134754, 134754, 134754, 134754, 134754, 538480]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08sYV5qt2brF",
        "colab_type": "text"
      },
      "source": [
        "## Load the Image Encoder\n",
        "\n",
        "Let's move to images. We will enconde every image in a 2048 vectors' space using the **InceptionV3 model (3)**. We can load the model pretrained on the **ImageNet (4)** using Keras. Since we want to encode the images and not recognize their content we will remove the output layer from the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yw845kF812o",
        "colab_type": "code",
        "outputId": "60fa81bc-9793-4b0d-81c5-def6164fe74e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "source": [
        "HEIGHT = 299\n",
        "WIDTH = 299\n",
        "OUTPUT_DIM = 2048\n",
        "\n",
        "encode_model = InceptionV3(weights='imagenet') # load the model\n",
        "encode_model = Model(encode_model.input, encode_model.layers[-2].output) # remove the output layer"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0902 09:48:29.731345 140260037138304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0902 09:48:29.765951 140260037138304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0902 09:48:29.778274 140260037138304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0902 09:48:29.814051 140260037138304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0902 09:48:29.815017 140260037138304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0902 09:48:33.070729 140260037138304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "W0902 09:48:33.345594 140260037138304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0902 09:48:34.204018 140260037138304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n",
            "96116736/96112376 [==============================] - 3s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR_YreJ92hR_",
        "colab_type": "text"
      },
      "source": [
        "## Preprocess the Images\n",
        "To preprocess the image we will load each image in a loop, resizing it to match the input size of 299x299 required by the Inception V3 model and feeding it to the model, then we will save the output inside a dictionary, where the key is again the image id. Since this process could take a long time (depending on your hardware and on the dataset's size) we will dump the dictionary on a local file using **pickle**, so the next time we can load this file instead of preprocessing again every image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PcgEa4ztPx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ENCODED_IMGS_FILE = \"features.pkl\"\n",
        "\n",
        "if(os.path.exists(ENCODED_IMGS_FILE)):\n",
        "  \n",
        "  with open(ENCODED_IMGS_FILE,\"rb\") as handle:\n",
        "    encoded_imgs = pickle.load(handle)\n",
        "\n",
        "else:\n",
        "\n",
        "  IMG_PATH = \"train2014/\"\n",
        "\n",
        "  imgs_id = set(captions_img)\n",
        "  encoded_imgs = {}\n",
        "\n",
        "  for i, img_id in enumerate(imgs_id):\n",
        "\n",
        "    img_file = 'COCO_train2014_' + '%012d.jpg' % img_id\n",
        "    img = load_img(IMG_PATH+img_file)\n",
        "    img = img.resize((WIDTH, HEIGHT), Image.ANTIALIAS)  \n",
        "    arr = img_to_array(img)\n",
        "\n",
        "    x = preprocess_input(arr)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "\n",
        "    features = encode_model.predict(x)\n",
        "    features = np.reshape(features, OUTPUT_DIM)\n",
        "\n",
        "    encoded_imgs[img_id]=features\n",
        "\n",
        "    with open(ENCODED_IMGS_FILE,\"wb\") as handle:\n",
        "      pickle.dump(encoded_imgs ,handle)      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvFkZN0WMvbN",
        "colab_type": "text"
      },
      "source": [
        "## Create the Train, Test and Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGDO8M6jIj0T",
        "colab_type": "code",
        "outputId": "9483c3f2-66ca-4efd-890e-339833ddaed5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "captions_text_train, captions_text_test, captions_img_train, captions_img_test = train_test_split(captions_text, captions_img, test_size=0.05, random_state=0)\n",
        "\n",
        "print(\"Sample in Train Set: %d \" % len(captions_text_train))\n",
        "print(\"Sample in Test Set: %d \" % len(captions_text_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample in Train Set: 4229 \n",
            "Sample in Test Set: 223 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsFm5Ol9QtEy",
        "colab_type": "text"
      },
      "source": [
        "## Create the Tokenizer\n",
        "It's time for tokenization ! This process consists in splitting the text in its constituent parts, called tokens. Many words in our text corpus can be very rare and such words will bring just noise to our model. So let's take only the words which appear at least 5 times in the entire text corpus. Define a function to get the vocabulary and another function to count the number of words that appear at least 5 times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whN2y2Jpx2Y1",
        "colab_type": "code",
        "outputId": "39cf4e9a-adbd-4660-99e5-9cc62043d80d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def get_vocab(captions):\n",
        "  \n",
        "  vocab = {}\n",
        "  \n",
        "  for caption in captions:\n",
        "    words = caption.split()\n",
        "    \n",
        "    for word in words:\n",
        "      \n",
        "      if(word in vocab):\n",
        "        vocab[word]+=1\n",
        "      else:\n",
        "        vocab[word]=1\n",
        "   \n",
        "  return vocab\n",
        "\n",
        "\n",
        "def count_top_words(captions, min_count=5):\n",
        "  \n",
        "  vocab = get_vocab(captions)\n",
        "  top_words = 0\n",
        "    \n",
        "  for word in vocab:\n",
        "    \n",
        "    if(vocab[word]>min_count):\n",
        "      top_words+=1\n",
        "      \n",
        "  return top_words\n",
        "  \n",
        "  \n",
        "VOCAB_SIZE = count_top_words(captions_text)\n",
        "print(\"Size of Corpus Vocabulary: %d \" % VOCAB_SIZE)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "698\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GynY3w5vNR-v",
        "colab_type": "text"
      },
      "source": [
        "Now we can use the Keras' Tokenizer class to perform tokenization on both train and test set, giving the vocabulary size with the *num_words* param."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22fnFyHHQweb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
        "tokenizer.fit_on_texts(captions_text)\n",
        "\n",
        "sequences_train = tokenizer.texts_to_sequences(captions_text_train)\n",
        "sequences_test = tokenizer.texts_to_sequences(captions_text_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tty8q8G9NrMV",
        "colab_type": "text"
      },
      "source": [
        "How long is the longest caption in the train set ? To build our model we need this information, let's get it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcL5Ex3AWXzx",
        "colab_type": "code",
        "outputId": "98e54d01-e290-409d-c651-2843c5109b97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "MAX_LEN = max(len(tokens) for tokens in sequences_train)\n",
        "print(\"Length of the longest caption in train set: %d \" % MAX_LEN)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "28\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gf81-h5OUiBZ"
      },
      "source": [
        "## Build a Generator\n",
        "\n",
        "Great ! We are almost done with data preprocessing, last steps are:\n",
        "* Create sequences of data, where, for each word in each caption, we will create a sequence with the word as target and all previous words as features.\n",
        "* We pad each features sequence to make every sequence the same length.\n",
        "* We create dummy variables for targets using One Hot Encoding.\n",
        "\n",
        "But there is an huge problem in the last step if we use the full dataset, in this case the resulting array will get really big. How big ?\n",
        "* We have 393407 captions in our train set, let's say that a caption is 10 words long in average plus the delimiters startseq and endseq, so we will we have 11 sequences for caption, we also have 8103 in our vocabulary, this means that the array containing encoded targets with One Hot Encoding will weight\n",
        "<br><br>\n",
        "  *393407 x 11 x 8103 x 4 = 140262184524 bytes ≈ 140 Gigabytes*\n",
        "<br><br>\n",
        "I multiplied by 4 because Keras uses float for One Hot Encoding, this is really too  much for my little tiny RAM. \n",
        "<br><br>\n",
        "What can we do ? A good solution is to use a generator and create our train samples on the fly during training, this will make the training time much longer but will save our RAM.<br>\n",
        "<br>\n",
        "Now it's also time to shuffle our sets, we will do this at every epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLcPAvTml8VL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_sequence(seq, caption_img, max_length, vocab_size):\n",
        "  \n",
        "  x1, x2, y = [], [], []\n",
        "\n",
        "    \n",
        "  for i in range(len(seq)-1):\n",
        "    x1.append(encoded_imgs[caption_img])\n",
        "    x2.append(pad_sequences([seq[:i+1]], maxlen=max_length)[0]) # pad the sequence\n",
        "    y.append(to_categorical([seq[i+1]], num_classes=vocab_size)[0]) # perform one hot encoding  \n",
        "      \n",
        "  return x1, x2, y\n",
        "\n",
        "\n",
        "def data_generator(sequences, captions_img, max_length, vocab_size, batch_size=None):\n",
        " \n",
        "  tot_samples = len(sequences)\n",
        "  \n",
        "  if(batch_size==None):\n",
        "    batch_size=tot_samples\n",
        "  \n",
        "  n_batches = int(tot_samples/batch_size)\n",
        "  \n",
        "  while 1:\n",
        "    \n",
        "    batch_start = 0\n",
        "    sequences, captions_img = shuffle(sequences, captions_img) # time to shuffle \n",
        "    \n",
        "    \n",
        "    for _ in range(n_batches):\n",
        "            \n",
        "      X1, X2, Y = [], [], [] \n",
        "      \n",
        "      batch_end = batch_start+batch_size\n",
        "            \n",
        "      if(batch_end>tot_samples):\n",
        "        batch_end = tot_samples\n",
        "        \n",
        "      sequences_batch = sequences[batch_start:batch_end] \n",
        "      captions_img_batch = captions_img[batch_start:batch_end]     \n",
        "                \n",
        "      for seq, caption_img in zip(sequences_batch, captions_img_batch):\n",
        "        \n",
        "        x1, x2, y = create_sequence(seq, caption_img, max_length, vocab_size)\n",
        "        \n",
        "        if(len(X1)==0):\n",
        "          X1, X2, Y = x1, x2, y\n",
        "        else:\n",
        "          X1, X2, Y = X1+x1, X2+x2, Y+y\n",
        "\n",
        "      batch_start+=batch_size\n",
        "        \n",
        "      yield [[np.array(X1), np.array(X2)],np.array(Y)]      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98bq3XeqabKq",
        "colab_type": "text"
      },
      "source": [
        "## Create the Embedding\n",
        "\n",
        "We can start building our Model ! The first layer of our Neural Network will perform Words Embedding (5), that will encode words in a N-dimensional vectors' space where related words are close. We will use a pretrained Word Embedding Model, the GloVe Model (6). Download the model and unzip it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlE_eXQsaaFj",
        "colab_type": "code",
        "outputId": "87f06f20-ecfe-4c4d-982f-774a0571fdbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        }
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-09-02 09:54:04--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2019-09-02 09:54:04--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2019-09-02 09:54:05--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M   101MB/s    in 8.2s    \n",
            "\n",
            "2019-09-02 09:54:13 (100 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjf_xdH6gaiM",
        "colab_type": "text"
      },
      "source": [
        "We got 4 files with different vectors' space: 50, 100, 200 and 300. For our model we will use the 100d embedding. For every row in the file we have the word and then the embedding values separated by spaces. Let's define a function to load the embedding inside a dictionary, where the key is the corresponding word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFSYIhEHdfFv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_embedding(filename):\n",
        "  \n",
        "  with open(filename) as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "  embedding = dict()\n",
        "  \n",
        "  for line in lines:\n",
        "    parts = line.split()\n",
        "    embedding[parts[0]] = np.asarray(parts[1:])\n",
        "    \n",
        "  return embedding\n",
        "\n",
        "raw_embedding = load_embedding('glove.6B.100d.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax0szD5ahKo9",
        "colab_type": "text"
      },
      "source": [
        "Now we can use the embedding dictionary to build an embedding weight matrix for our text corpus, let's define another function for this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_K1yn3nel4j",
        "colab_type": "code",
        "outputId": "89e2fa38-df86-43dd-ae42-b71342041dee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def get_weight_matrix(embedding, index_word, vocab_size):\n",
        "    \n",
        "  weight_matrix = np.zeros((vocab_size, 100))\n",
        "\n",
        "  for i in range(vocab_size):\n",
        "    word = index_word[i+1]\n",
        "    vector = embedding.get(word)\n",
        "    if vector is not None:\n",
        "      weight_matrix[i] = vector\n",
        "      \n",
        "  return weight_matrix\n",
        "\n",
        "embedding_vectors = get_weight_matrix(raw_embedding, tokenizer.index_word, VOCAB_SIZE)\n",
        "embedding_vectors.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(698, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hc6NJegVd5xh",
        "colab_type": "text"
      },
      "source": [
        "## Buld the Neural Network\n",
        "Time to build the Neural Nework ! Let's define it's architecture using Keras' Functional API. Our Neural Nework has two inputs, the 2048 dimensions encoded image and the text-emcpded sequence. It's pretty easy to overfit for such problem, so we will use Dropout to reduce the risk of overfitting. We don't want to change the weights of our embedding layer during training, so we will set the *trainable* parameters to false."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdQmcdN5d77T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMBEDDING_DIM = 100\n",
        "\n",
        "inputs1 = Input(shape=(OUTPUT_DIM,))\n",
        "fe1 = Dropout(0.5)(inputs1)\n",
        "fe2 = Dense(256, activation='relu')(fe1)\n",
        "inputs2 = Input(shape=(MAX_LEN,))\n",
        "se1 = Embedding(VOCAB_SIZE, EMBEDDING_DIM, mask_zero=True, weights=[embedding_vectors], trainable=False)(inputs2)\n",
        "se2 = Dropout(0.5)(se1)\n",
        "se3 = LSTM(256)(se2)\n",
        "decoder1 = add([fe2, se3])\n",
        "decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "outputs = Dense(VOCAB_SIZE, activation='softmax')(decoder2)\n",
        "caption_model = Model(inputs=[inputs1, inputs2], outputs=outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-YiDq6uifOg",
        "colab_type": "text"
      },
      "source": [
        "Great, we can compile the model now, since this is a Multi-class Classification problem we will use Categorical Crossentropy as Loss Function and **ADAM (7)** as optimization algorithm, which should perform better than a simple Stochastic Gradient Descent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZX5vVqteSwL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "caption_model.compile(loss='categorical_crossentropy', optimizer=\"adam\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Oa4Uz-OlqwZ",
        "colab_type": "text"
      },
      "source": [
        "## Fit and Evaluate with Generators\n",
        "Before starting training we define some callbacks that Keras will execute after each epoch. On the first callback we will call a custom function that will generate a caption for a picture randomly picked from the test set. First define the function to generate the caption, using the sampling technique:\n",
        "1. Provide the image and the *start of sequence* delimiter as input to the network.\n",
        "2. Sample from the output the word with the highest probability.\n",
        "3. Add the word sampled to the sequence and use this as new input for the network together with the image.\n",
        "4. Repeat step 2 and 3 until sampling the *end of sequence* delimiter or until the maximum length for the caption has been reached.\n",
        "\n",
        "Let's do it !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8S6w2HM6mic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_caption(x1):\n",
        "  \n",
        "  caption = \"startseq\"\n",
        "          \n",
        "  for i in range(MAX_LEN):\n",
        "    \n",
        "    seq = tokenizer.texts_to_sequences([caption])\n",
        "        \n",
        "    x2 = pad_sequences(seq, maxlen=MAX_LEN)\n",
        "        \n",
        "    y = caption_model.predict([x1,x2], verbose=0)\n",
        "    word = tokenizer.index_word[np.argmax(y)]\n",
        "        \n",
        "    caption+=\" \"+word\n",
        "    \n",
        "    if word == \"endseq\":\n",
        "      break\n",
        "\n",
        "  return caption\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwczQTNU6rEG",
        "colab_type": "text"
      },
      "source": [
        "And then define the callback function and create the callback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4sRsJNGYsye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def caption_on_epoch(epoch, _):\n",
        "  \n",
        "  test_sample = randint(0, len(captions_img_test)-1)\n",
        "\n",
        "  x1 = encoded_imgs[captions_img_test[test_sample]]\n",
        "  x1 = np.expand_dims(x1, axis=0)\n",
        "\n",
        "  print(\"GENERATED CAPTION: \"+generate_caption(x1))\n",
        "  print(\"REFERENCE CAPTION: \"+captions_text_test[test_sample])\n",
        "  \n",
        "epoch_end_callback = LambdaCallback(on_epoch_end=caption_on_epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAl6T6rGkPKo",
        "colab_type": "text"
      },
      "source": [
        "Define a second callback that will perform early stopping if the log loss on the validation set doesn't improve at least of 0.001 in 5 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_af_vr-3Qte",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "earlyStopping = EarlyStopping(min_delta=0.001, patience=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QyDw535keO-",
        "colab_type": "text"
      },
      "source": [
        "And two more callbaks to backup our model, one will store our model after each epoch and the other will save only the best model, that is the model that got the lowest loss on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9X6_RVikf7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lastModelCheckpoint = ModelCheckpoint(\"caption_model.h5\")\n",
        "bestModelCheckpoint = ModelCheckpoint(\"best_caption_model.h5\", save_best_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79uN0Mijkpk7",
        "colab_type": "text"
      },
      "source": [
        "We can now star training for 20 epochs with 32 samples for every step. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fK6InM05RCs-",
        "colab_type": "code",
        "outputId": "2cad6666-90a9-4449-ee01-a7b32d562bf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "generator_train = data_generator(sequences_train, captions_img_train, MAX_LEN, VOCAB_SIZE, batch_size=BATCH_SIZE)\n",
        "generator_test = data_generator(sequences_test, captions_img_test, MAX_LEN, VOCAB_SIZE, batch_size=BATCH_SIZE)\n",
        "\n",
        "caption_model.fit_generator(generator_train, epochs=20, steps_per_epoch=len(captions_text_train)//BATCH_SIZE, \n",
        "                            validation_data= generator_test, validation_steps=len(captions_text_test)//BATCH_SIZE,\n",
        "                            callbacks=[earlyStopping, epoch_end_callback, lastModelCheckpoint, bestModelCheckpoint])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "132/132 [==============================] - 19s 143ms/step - loss: 4.4173 - val_loss: 3.9421\n",
            "GENERATED CAPTION: startseq a a a a a a a a a a endseq\n",
            "REFERENCE CAPTION: startseq a man sits at a park with his bike endseq\n",
            "Epoch 2/20\n",
            "132/132 [==============================] - 15s 117ms/step - loss: 3.7612 - val_loss: 3.4713\n",
            "GENERATED CAPTION: startseq a woman with a bench on a endseq\n",
            "REFERENCE CAPTION: startseq a closeup of a young person holding a tennis racket endseq\n",
            "Epoch 3/20\n",
            "132/132 [==============================] - 15s 117ms/step - loss: 3.4124 - val_loss: 3.2248\n",
            "GENERATED CAPTION: startseq a bathroom with a sink and a sink endseq\n",
            "REFERENCE CAPTION: startseq the framed picture shows a large tidy kitchen endseq\n",
            "Epoch 4/20\n",
            "132/132 [==============================] - 15s 117ms/step - loss: 3.1789 - val_loss: 3.0687\n",
            "GENERATED CAPTION: startseq a man riding a bike with a bicycle endseq\n",
            "REFERENCE CAPTION: startseq two adults and a child riding bicycles on the side of a road endseq\n",
            "Epoch 5/20\n",
            "132/132 [==============================] - 15s 115ms/step - loss: 2.9976 - val_loss: 2.9666\n",
            "GENERATED CAPTION: startseq a bicycle is on a bike endseq\n",
            "REFERENCE CAPTION: startseq a old world coastal city with sailing ships locked in the frozen waters endseq\n",
            "Epoch 6/20\n",
            "132/132 [==============================] - 15s 116ms/step - loss: 2.8547 - val_loss: 2.8411\n",
            "GENERATED CAPTION: startseq a cat standing in the bathroom with a sink endseq\n",
            "REFERENCE CAPTION: startseq a black cat is in a white bathroom sink endseq\n",
            "Epoch 7/20\n",
            "132/132 [==============================] - 15s 115ms/step - loss: 2.7361 - val_loss: 2.7976\n",
            "GENERATED CAPTION: startseq a kitchen with a sink and a sink endseq\n",
            "REFERENCE CAPTION: startseq a kitchen that is being remodeled with cabinet doors off endseq\n",
            "Epoch 8/20\n",
            "132/132 [==============================] - 15s 114ms/step - loss: 2.6391 - val_loss: 2.7132\n",
            "GENERATED CAPTION: startseq a bathroom with a toilet and a sink endseq\n",
            "REFERENCE CAPTION: startseq small renovated residential bathroom using glass and chrome endseq\n",
            "Epoch 9/20\n",
            "132/132 [==============================] - 15s 115ms/step - loss: 2.5370 - val_loss: 2.6971\n",
            "GENERATED CAPTION: startseq a kitchen with a white refrigerator and refrigerator endseq\n",
            "REFERENCE CAPTION: startseq a white kitchen with a large refrigerator freezer combo endseq\n",
            "Epoch 10/20\n",
            "132/132 [==============================] - 15s 115ms/step - loss: 2.4677 - val_loss: 2.7391\n",
            "GENERATED CAPTION: startseq a woman with a dog umbrella a woman down the street endseq\n",
            "REFERENCE CAPTION: startseq a lady is standing in the rain possibly looking off to the road  endseq\n",
            "Epoch 11/20\n",
            "132/132 [==============================] - 15s 115ms/step - loss: 2.3951 - val_loss: 2.6646\n",
            "GENERATED CAPTION: startseq a kitchen with a white refrigerator and a microwave endseq\n",
            "REFERENCE CAPTION: startseq a fridge covered in photos and magnets with a brightly colored kitchen floor endseq\n",
            "Epoch 12/20\n",
            "132/132 [==============================] - 15s 115ms/step - loss: 2.3202 - val_loss: 2.7156\n",
            "GENERATED CAPTION: startseq a man riding a motorcycle on a road endseq\n",
            "REFERENCE CAPTION: startseq a man is on a yellow motorcycle at night with the background scene blurred endseq\n",
            "Epoch 13/20\n",
            "132/132 [==============================] - 15s 115ms/step - loss: 2.2536 - val_loss: 2.6116\n",
            "GENERATED CAPTION: startseq a kitchen with a white floor that needs endseq\n",
            "REFERENCE CAPTION: startseq a white kitchen with a tile floor needing repair endseq\n",
            "Epoch 14/20\n",
            "132/132 [==============================] - 15s 115ms/step - loss: 2.1978 - val_loss: 2.6802\n",
            "GENERATED CAPTION: startseq a group of people standing on a field endseq\n",
            "REFERENCE CAPTION: startseq men and women outside playing a game of frisbee endseq\n",
            "Epoch 15/20\n",
            "132/132 [==============================] - 15s 114ms/step - loss: 2.1373 - val_loss: 2.7092\n",
            "GENERATED CAPTION: startseq a bathroom with a toilet and a sink endseq\n",
            "REFERENCE CAPTION: startseq small renovated residential bathroom using glass and chrome endseq\n",
            "Epoch 16/20\n",
            "132/132 [==============================] - 15s 116ms/step - loss: 2.0789 - val_loss: 2.7672\n",
            "GENERATED CAPTION: startseq a young child holding a tennis racket endseq\n",
            "REFERENCE CAPTION: startseq a closeup of a young person holding a tennis racket endseq\n",
            "Epoch 17/20\n",
            "132/132 [==============================] - 15s 115ms/step - loss: 2.0323 - val_loss: 2.6861\n",
            "GENERATED CAPTION: startseq a bathroom with a toilet and a toilet endseq\n",
            "REFERENCE CAPTION: startseq a bathroom showing a toilet toilet paper and biohazard sign endseq\n",
            "Epoch 18/20\n",
            "132/132 [==============================] - 15s 116ms/step - loss: 1.9814 - val_loss: 2.7054\n",
            "GENERATED CAPTION: startseq a man standing in a bathroom stall on the toilet endseq\n",
            "REFERENCE CAPTION: startseq a man is standing on a toilet peeking into the nest stall endseq\n",
            "Epoch 19/20\n",
            "132/132 [==============================] - 15s 114ms/step - loss: 1.9347 - val_loss: 2.7111\n",
            "GENERATED CAPTION: startseq a man is holding a picture of a kitchen endseq\n",
            "REFERENCE CAPTION: startseq an orange and white cat sleeping inside of a bathroom sink endseq\n",
            "Epoch 20/20\n",
            "132/132 [==============================] - 15s 116ms/step - loss: 1.8896 - val_loss: 2.7203\n",
            "GENERATED CAPTION: startseq a kitchen with a tile floor that needs a dishwasher endseq\n",
            "REFERENCE CAPTION: startseq a white kitchen with a tile floor needing repair endseq\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f900352cb38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnD_QcAgkzKO",
        "colab_type": "text"
      },
      "source": [
        "Let's execute 10 more epochs on 64 sample per step, lowering the learning rate at 0.0001, this makes sense since when we are close to the minimum we want to take smaller steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFwcp5Xmgim8",
        "colab_type": "code",
        "outputId": "8b598ae5-915b-43d5-ee73-8198b760f589",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726
        }
      },
      "source": [
        "#caption_model.optimizer.lr = 1e-4\n",
        "caption_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "caption_model.optmizer = Adam(lr=1e-4)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "generator_train = data_generator(sequences_train, captions_img_train, MAX_LEN, VOCAB_SIZE, batch_size=BATCH_SIZE)\n",
        "generator_test = data_generator(sequences_test, captions_img_test, MAX_LEN, VOCAB_SIZE, batch_size=BATCH_SIZE)\n",
        "\n",
        "caption_model.fit_generator(generator_train, epochs=10, steps_per_epoch=len(captions_text_train)//BATCH_SIZE, \n",
        "                            validation_data= generator_test, validation_steps=len(captions_text_test)//BATCH_SIZE,\n",
        "                            callbacks=[earlyStopping, epoch_end_callback, lastModelCheckpoint, bestModelCheckpoint])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "66/66 [==============================] - 13s 193ms/step - loss: 1.8312 - val_loss: 2.7596\n",
            "GENERATED CAPTION: startseq a scooter parked by a scooter with a in the background endseq\n",
            "REFERENCE CAPTION: startseq there are scooters and bicycles parked by a wall endseq\n",
            "Epoch 2/10\n",
            "66/66 [==============================] - 9s 141ms/step - loss: 1.7490 - val_loss: 2.7287\n",
            "GENERATED CAPTION: startseq a bathroom with a toilet and a sink endseq\n",
            "REFERENCE CAPTION: startseq an empty bathroom with a sink and toilette underneath a hand drying towel endseq\n",
            "Epoch 3/10\n",
            "66/66 [==============================] - 9s 140ms/step - loss: 1.7176 - val_loss: 2.7907\n",
            "GENERATED CAPTION: startseq a woman sitting in a car with a black and white cat endseq\n",
            "REFERENCE CAPTION: startseq a lady driving her car with a black and white cat in her lap endseq\n",
            "Epoch 4/10\n",
            "66/66 [==============================] - 9s 141ms/step - loss: 1.6891 - val_loss: 2.8145\n",
            "GENERATED CAPTION: startseq a person holding a baby on a bed endseq\n",
            "REFERENCE CAPTION: startseq a man is laying on a bed as a furry critter moves bedside him and almost drops a filter pitcher endseq\n",
            "Epoch 5/10\n",
            "66/66 [==============================] - 9s 139ms/step - loss: 1.6645 - val_loss: 2.8252\n",
            "GENERATED CAPTION: startseq a kitchen with a white refrigerator and a white fridge endseq\n",
            "REFERENCE CAPTION: startseq a white kitchen with a large refrigerator freezer combo endseq\n",
            "Epoch 6/10\n",
            "66/66 [==============================] - 9s 138ms/step - loss: 1.6251 - val_loss: 2.8087\n",
            "GENERATED CAPTION: startseq a bathroom with a toilet and a chair endseq\n",
            "REFERENCE CAPTION: startseq a bathroom has swinging saloon style stall doors endseq\n",
            "Epoch 7/10\n",
            "66/66 [==============================] - 9s 138ms/step - loss: 1.6118 - val_loss: 2.8415\n",
            "GENERATED CAPTION: startseq a man standing in a bathroom stall on a toilet endseq\n",
            "REFERENCE CAPTION: startseq a man is standing up on a toilet peeking over at someone wearing dress shoes endseq\n",
            "Epoch 8/10\n",
            "66/66 [==============================] - 9s 140ms/step - loss: 1.5805 - val_loss: 2.8642\n",
            "GENERATED CAPTION: startseq a man standing next to a car holding a black car endseq\n",
            "REFERENCE CAPTION: startseq an older man standing next to an old fashioned car endseq\n",
            "Epoch 9/10\n",
            "66/66 [==============================] - 9s 138ms/step - loss: 1.5604 - val_loss: 2.9617\n",
            "GENERATED CAPTION: startseq a bathroom with a toilet and a wall endseq\n",
            "REFERENCE CAPTION: startseq a bathroom showing a toilet toilet paper and biohazard sign endseq\n",
            "Epoch 10/10\n",
            "66/66 [==============================] - 9s 141ms/step - loss: 1.5337 - val_loss: 2.9088\n",
            "GENERATED CAPTION: startseq a bathroom with a toilet and a sink endseq\n",
            "REFERENCE CAPTION: startseq small renovated residential bathroom using glass and chrome endseq\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9002226fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSS2UCNmlXMt",
        "colab_type": "text"
      },
      "source": [
        "Our model is ready ! Let's compute the log loss on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbfuVBGDHOC7",
        "colab_type": "code",
        "outputId": "edfd77b1-1c21-40ed-dc73-4fba2af69918",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "generator_test = data_generator(sequences_test, captions_img_test, MAX_LEN, VOCAB_SIZE, batch_size=1)\n",
        "caption_model.evaluate_generator(generator_test, steps=len(sequences_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.8195897356507285"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28qUVHaalcFN",
        "colab_type": "text"
      },
      "source": [
        "This metric doesn't help so much, because our goal isn't to predict the exact word for each caption but to generate a caption that could generally describe the content of the image. We can use a better metric for this, the BLEU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaUaWaanPw52",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate with the BLEU Score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5L4Uhv_lw1p",
        "colab_type": "text"
      },
      "source": [
        "The BLEU (2), which stand for Bilingual Evaluation Understudy, is a evaluation metric popular in the field on Machine Translation but it can be used to evaluate every kind of text generated tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xLPj62dIv4x",
        "colab_type": "code",
        "outputId": "24002810-67d9-40eb-fdc0-dac3fe915f0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "generated_captions = []\n",
        "reference_captions = []\n",
        "\n",
        "for img_id in captions_img_test:\n",
        "  x1 = encoded_imgs[img_id]\n",
        "  x1 = np.expand_dims(x1, axis=0)\n",
        "  generated_captions.append(generate_caption(x1))\n",
        "  reference_captions.append([caption for caption in img_captions[img_id]])\n",
        "  \n",
        "print('BLEU-1: %f' % corpus_bleu(reference_captions, generated_captions, weights=(1.0, 0, 0, 0)))\n",
        "print('BLEU-2: %f' % corpus_bleu(reference_captions, generated_captions, weights=(0.5, 0.5, 0, 0)))\n",
        "print('BLEU-3: %f' % corpus_bleu(reference_captions, generated_captions, weights=(0.3, 0.3, 0.3, 0)))\n",
        "print('BLEU-4: %f' % corpus_bleu(reference_captions, generated_captions, weights=(0.25, 0.25, 0.25, 0.25)))  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU-1: 0.932487\n",
            "BLEU-2: 0.874970\n",
            "BLEU-3: 0.826569\n",
            "BLEU-4: 0.760680\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qNJUXQsip32",
        "colab_type": "text"
      },
      "source": [
        "## Test the Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HW4AxAtVbWuy",
        "colab_type": "text"
      },
      "source": [
        "Let's test the quality of our Neural Image Captioning Model on some scenes of famous movies. First define a function that preprocess the image and generate the caption using the function we defined before:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30EeL3hg68MK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def caption_it(img):\n",
        "  \n",
        "  img = img.resize((WIDTH, HEIGHT), Image.ANTIALIAS)  \n",
        "  img = img_to_array(img)\n",
        "  img = preprocess_input(img)\n",
        "  img = np.expand_dims(img, axis=0)\n",
        "    \n",
        "  x1 = encode_model.predict(img)\n",
        "  x1 = x1.reshape((1, OUTPUT_DIM))\n",
        "      \n",
        "  caption = generate_caption(x1)\n",
        "  \n",
        "  return \" \".join(caption.split()[1:-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TnyebIs7Esg",
        "colab_type": "text"
      },
      "source": [
        "And now let's play a bit with our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMB99W7Th24z",
        "colab_type": "code",
        "outputId": "7ab220f7-1c05-4770-9a9f-f8f970fb3e21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        }
      },
      "source": [
        "!wget https://cdn.pixabay.com/photo/2017/02/20/18/03/cat-2083492_960_720.jpg\n",
        "  \n",
        "img = Image.open(\"cat-2083492_960_720.jpg\")\n",
        "caption = caption_it(img)\n",
        "display(img)\n",
        "print(\"I see \"+caption)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-09-01 13:03:20--  https://cdn.pixabay.com/photo/2017/02/20/18/03/cat-2083492_960_720.jpg\n",
            "Resolving cdn.pixabay.com (cdn.pixabay.com)... 104.18.82.97, 104.18.141.87, 2606:4700::6812:8d57, ...\n",
            "Connecting to cdn.pixabay.com (cdn.pixabay.com)|104.18.82.97|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 143288 (140K) [image/jpeg]\n",
            "Saving to: ‘cat-2083492_960_720.jpg’\n",
            "\n",
            "\rcat-2083492_960_720   0%[                    ]       0  --.-KB/s               \rcat-2083492_960_720 100%[===================>] 139.93K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2019-09-01 13:03:20 (13.0 MB/s) - ‘cat-2083492_960_720.jpg’ saved [143288/143288]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-01666d9e240d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wget https://cdn.pixabay.com/photo/2017/02/20/18/03/cat-2083492_960_720.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cat-2083492_960_720.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcaption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcaption_it\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"I see \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcaption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-c6ef7b789236>\u001b[0m in \u001b[0;36mcaption_it\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_DIM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mcaption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_caption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'generate_caption' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYH_f58FekwH",
        "colab_type": "text"
      },
      "source": [
        "Our model works good enough, we are ready to [deploy it](https://github.com/gfgullo/Pic2Speech/blob/master/jupyter%20notebooks/azure_model_deploy.ipynb) !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqaXMWjjOcQ4",
        "colab_type": "text"
      },
      "source": [
        "<a id='references'></a>\n",
        "## References\n",
        "1. [Show and Tell: A Neural Image Caption Generator by Vinyals et al](https://arxiv.org/pdf/1411.4555.pdf)\n",
        "2. [BLEU: a Method for Automatic Evaluation of Machine Translation by Papineni et al](https://www.aclweb.org/anthology/P02-1040)\n",
        "3. [Rethinking the Inception Architecture for Computer Vision by Szegedy](https://arxiv.org/pdf/1512.00567.pdf)\n",
        "4. [ImageNet: A Large-Scale Hierarchical Image Database by Deng](http://www.image-net.org/papers/imagenet_cvpr09.pdf)\n",
        "5. [Distributed Representations of Words and Phrases\n",
        "and their Compositionality by Mikolov et al](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
        "6. [GloVe: Global Vectors for Word Representation by Pennington et al](https://nlp.stanford.edu/pubs/glove.pdf)\n",
        "7. [Adam: A Method for Stochastic Optimization by Kingma et al](https://arxiv.org/pdf/1412.6980.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWM6n8_-0HAB",
        "colab_type": "text"
      },
      "source": [
        "## Resources\n",
        "* https://www.tensorflow.org/beta/tutorials/text/image_captioning\n",
        "* https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/\n",
        "* https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_10_4_captioning.ipynb\n",
        "* https://github.com/hlamba28/Automatic-Image-Captioning/blob/master/Automatic%20Image%20Captioning.ipynb"
      ]
    }
  ]
}